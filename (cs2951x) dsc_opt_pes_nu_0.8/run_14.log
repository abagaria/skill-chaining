Goal position:  [0. 8.]
Training skill chaining agent from scratch with a subgoal reward 300.0
MDP InitState =  x: 0.0005343962002579694	y: -0.009862241832153201	theta: 0.05565404666389104	xdot: 0.06438169035442923	ydot: -0.04471083463093509	thetadot: -0.19016111359377297	terminal: False

Initializing skill chaining with option_timeout=True, seed=24
Creating global_option with enable_timeout=True
Creating overall_goal_policy_option with enable_timeout=True

Creating GlobalDQN with lr=0.0001 and ddqn=True and buffer_sz=1000000

|-> (SkillChaining::skill_chaining): call
|-> episode: 0
  |-> step_number: 0
  |-> step_number: 100
  |-> step_number: 200
  |-> step_number: 300
  |-> step_number: 400
  |-> step_number: 500
  |-> step_number: 600
  |-> step_number: 700
  |-> step_number: 800
  |-> step_number: 900
Episode 0	Average Score: -1000.00	Duration: 1000.00 steps	GO Eps: 0.99
Episode 0	Average Score: -1000.00	Duration: 1000.00 steps	GO Eps: 0.99
|-> episode: 1
  |-> step_number: 0
  |-> step_number: 100
  |-> step_number: 200
  |-> step_number: 300
  |-> step_number: 400
  |-> step_number: 500
  |-> step_number: 600
  |-> step_number: 700
  |-> step_number: 800
  |-> step_number: 900
Episode 1	Average Score: -1000.00	Duration: 1000.00 steps	GO Eps: 0.98
|-> episode: 2
  |-> step_number: 0
  |-> step_number: 100
  |-> step_number: 200
  |-> step_number: 300
  |-> step_number: 400
  |-> step_number: 500
  |-> step_number: 600
  |-> step_number: 700
  |-> step_number: 800
  |-> step_number: 900
Episode 2	Average Score: -1000.00	Duration: 1000.00 steps	GO Eps: 0.97
|-> episode: 3
  |-> step_number: 0
  |-> step_number: 100
  |-> step_number: 200
  |-> step_number: 300
  |-> step_number: 400
  |-> step_number: 500
  |-> step_number: 600
global_option execution successful
    |-> (Option::train): call (train overall_goal_policy_option)
      |-> num_goal_hits: 1, num_subgoal_hits_required: 5 
Episode 3	Average Score: -908.25	Duration: 908.50 steps	GO Eps: 0.96
|-> episode: 4
  |-> step_number: 0
  |-> step_number: 100
  |-> step_number: 200
  |-> step_number: 300
  |-> step_number: 400
global_option execution successful
    |-> (Option::train): call (train overall_goal_policy_option)
      |-> num_goal_hits: 2, num_subgoal_hits_required: 5 
Episode 4	Average Score: -808.20	Duration: 808.60 steps	GO Eps: 0.96
|-> episode: 5
  |-> step_number: 0
  |-> step_number: 100
  |-> step_number: 200
  |-> step_number: 300
  |-> step_number: 400
  |-> step_number: 500
  |-> step_number: 600
  |-> step_number: 700
  |-> step_number: 800
global_option execution successful
    |-> (Option::train): call (train overall_goal_policy_option)
      |-> num_goal_hits: 3, num_subgoal_hits_required: 5 
Episode 5	Average Score: -810.50	Duration: 811.00 steps	GO Eps: 0.95
|-> episode: 6
  |-> step_number: 0
  |-> step_number: 100
  |-> step_number: 200
  |-> step_number: 300
  |-> step_number: 400
  |-> step_number: 500
global_option execution successful
    |-> (Option::train): call (train overall_goal_policy_option)
      |-> num_goal_hits: 4, num_subgoal_hits_required: 5 
Episode 6	Average Score: -775.71	Duration: 776.29 steps	GO Eps: 0.95
|-> episode: 7
  |-> step_number: 0
  |-> step_number: 100
  |-> step_number: 200
  |-> step_number: 300
  |-> step_number: 400
  |-> step_number: 500
  |-> step_number: 600
  |-> step_number: 700
  |-> step_number: 800
global_option execution successful
    |-> (Option::train): call (train overall_goal_policy_option)
      |-> num_goal_hits: 5, num_subgoal_hits_required: 5 
      |-> No negative examples...Adding 5 now!
overall_goal_policy_option execution successful
overall_goal_policy_option execution successful
overall_goal_policy_option execution successful
overall_goal_policy_option execution successful
overall_goal_policy_option execution successful

Creating GlobalDQN with lr=0.0001 and ddqn=True and buffer_sz=1000000

Initializing new option node with q value 0.0
Creating option_1
Creating option_1 with enable_timeout=True
Episode 7	Average Score: -785.38	Duration: 786.00 steps	GO Eps: 0.94
|-> episode: 8
  |-> step_number: 0
  |-> step_number: 100
  |-> step_number: 200
  |-> step_number: 300
  |-> step_number: 400
  |-> step_number: 500
  |-> step_number: 600
  |-> step_number: 700
  |-> step_number: 800
  |-> step_number: 900
Episode 8	Average Score: -809.22	Duration: 809.78 steps	GO Eps: 0.93
|-> episode: 9
  |-> step_number: 0
  |-> step_number: 100
  |-> step_number: 200
  |-> step_number: 300
  |-> step_number: 400
  |-> step_number: 500
  |-> step_number: 600
  |-> step_number: 700
  |-> step_number: 800
  |-> step_number: 900
Episode 9	Average Score: -828.30	Duration: 828.80 steps	GO Eps: 0.92
|-> episode: 10
  |-> step_number: 0
    |-> (Option::train): call (train option_1)
      |-> num_goal_hits: 1, num_subgoal_hits_required: 5 
  |-> step_number: 200
  |-> step_number: 300
  |-> step_number: 400
  |-> step_number: 500
  |-> step_number: 600
  |-> step_number: 700
overall_goal_policy_option execution successful
Episode 10	Average Score: -815.20	Duration: 815.80 steps	GO Eps: 0.91
Episode 10	Average Score: -815.20	Duration: 815.80 steps	GO Eps: 0.91
|-> episode: 11
  |-> step_number: 0
  |-> step_number: 100
  |-> step_number: 200
  |-> step_number: 300
  |-> step_number: 400
  |-> step_number: 500
  |-> step_number: 600
  |-> step_number: 700
  |-> step_number: 800
  |-> step_number: 900
Episode 11	Average Score: -815.20	Duration: 815.80 steps	GO Eps: 0.90
|-> episode: 12
  |-> step_number: 0
  |-> step_number: 100
  |-> step_number: 200
    |-> (Option::train): call (train option_1)
      |-> num_goal_hits: 2, num_subgoal_hits_required: 5 
overall_goal_policy_option execution successful
Episode 12	Average Score: -744.30	Duration: 745.00 steps	GO Eps: 0.90
|-> episode: 13
  |-> step_number: 0
  |-> step_number: 100
  |-> step_number: 200
    |-> (Option::train): call (train option_1)
      |-> num_goal_hits: 3, num_subgoal_hits_required: 5 
overall_goal_policy_option execution successful
Episode 13	Average Score: -707.50	Duration: 708.20 steps	GO Eps: 0.89
|-> episode: 14
  |-> step_number: 0
  |-> step_number: 100
  |-> step_number: 200
  |-> step_number: 300
  |-> step_number: 400
  |-> step_number: 500
  |-> step_number: 600
  |-> step_number: 700
  |-> step_number: 800
  |-> step_number: 900
Episode 14	Average Score: -766.70	Duration: 767.30 steps	GO Eps: 0.88
|-> episode: 15
  |-> step_number: 0
  |-> step_number: 100
  |-> step_number: 200
  |-> step_number: 300
  |-> step_number: 400
  |-> step_number: 500
  |-> step_number: 600
  |-> step_number: 700
  |-> step_number: 800
  |-> step_number: 900
Episode 15	Average Score: -784.50	Duration: 785.00 steps	GO Eps: 0.87
|-> episode: 16
  |-> step_number: 0
  |-> step_number: 100
  |-> step_number: 200
  |-> step_number: 300
  |-> step_number: 400
  |-> step_number: 500
  |-> step_number: 600
  |-> step_number: 700
  |-> step_number: 800
  |-> step_number: 900
Episode 16	Average Score: -827.80	Duration: 828.20 steps	GO Eps: 0.86
|-> episode: 17
  |-> step_number: 0
  |-> step_number: 100
  |-> step_number: 200
  |-> step_number: 300
  |-> step_number: 400
  |-> step_number: 500
  |-> step_number: 600
    |-> (Option::train): call (train option_1)
      |-> num_goal_hits: 4, num_subgoal_hits_required: 5 
overall_goal_policy_option execution successful
Episode 17	Average Score: -815.10	Duration: 815.50 steps	GO Eps: 0.86
|-> episode: 18
  |-> step_number: 0
  |-> step_number: 100
  |-> step_number: 200
  |-> step_number: 300
  |-> step_number: 400
  |-> step_number: 500
  |-> step_number: 600
  |-> step_number: 700
  |-> step_number: 800
  |-> step_number: 900
Episode 18	Average Score: -815.10	Duration: 815.50 steps	GO Eps: 0.85
|-> episode: 19
  |-> step_number: 0
  |-> step_number: 100
  |-> step_number: 200
  |-> step_number: 300
  |-> step_number: 400
    |-> (Option::train): call (train option_1)
      |-> num_goal_hits: 5, num_subgoal_hits_required: 5 
      |-> No negative examples...Adding 5 now!
option_1 execution successful
option_1 execution successful
option_1 execution successful

Creating GlobalDQN with lr=0.0001 and ddqn=True and buffer_sz=1000000

Initializing new option node with q value 0.0
Creating option_2
Creating option_2 with enable_timeout=True
option_1 execution successful
option_1 execution successful
option_1 execution successful
option_1 execution successful
  |-> step_number: 600
option_1 execution successful
option_1 execution successful
option_1 execution successful
option_1 execution successful
option_1 execution successful
Episode 19	Average Score: -815.10	Duration: 815.50 steps	GO Eps: 0.84
|-> episode: 20
  |-> step_number: 0
  |-> step_number: 100
    |-> (Option::train): call (train option_2)
      |-> num_goal_hits: 1, num_subgoal_hits_required: 5 
  |-> step_number: 300
option_1 execution successful
overall_goal_policy_option execution successful
Episode 20	Average Score: -813.40	Duration: 813.80 steps	GO Eps: 0.83
Episode 20	Average Score: -813.40	Duration: 813.80 steps	GO Eps: 0.83
|-> episode: 21
  |-> step_number: 0
  |-> step_number: 100
  |-> step_number: 200
  |-> step_number: 300
    |-> (Option::train): call (train option_2)
      |-> num_goal_hits: 2, num_subgoal_hits_required: 5 
  |-> step_number: 500
  |-> step_number: 600
[option_1]: x: 1.0890242668790777	y: 8.411362393879594	theta: 17.947699529282186	xdot: -0.16936910272562625	ydot: -1.4392933483395336	thetadot: 0.1864771874488865	terminal: False
 is_terminal() but not term_true()
Episode 21	Average Score: -804.90	Duration: 805.40 steps	GO Eps: 0.82
|-> episode: 22
  |-> step_number: 0
  |-> step_number: 100
  |-> step_number: 200
    |-> (Option::train): call (train option_2)
      |-> num_goal_hits: 3, num_subgoal_hits_required: 5 
  |-> step_number: 600
  |-> step_number: 800
option_1 execution successful
option_1 execution successful
overall_goal_policy_option execution successful
Episode 22	Average Score: -869.80	Duration: 870.30 steps	GO Eps: 0.81
|-> episode: 23
  |-> step_number: 0
  |-> step_number: 100
overall_goal_policy_option execution successful
Episode 23	Average Score: -857.80	Duration: 858.30 steps	GO Eps: 0.81
|-> episode: 24
  |-> step_number: 0
  |-> step_number: 100
  |-> step_number: 200
  |-> step_number: 300
  |-> step_number: 400
    |-> (Option::train): call (train option_2)
      |-> num_goal_hits: 4, num_subgoal_hits_required: 5 
  |-> step_number: 600
option_1 execution successful
Episode 24	Average Score: -857.80	Duration: 858.30 steps	GO Eps: 0.80
|-> episode: 25
  |-> step_number: 0
    |-> (Option::train): call (train option_2)
      |-> num_goal_hits: 5, num_subgoal_hits_required: 5 
      |-> No negative examples...Adding 5 now!
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful

Creating GlobalDQN with lr=0.0001 and ddqn=True and buffer_sz=1000000

Initializing new option node with q value 0.0
Creating option_3
Creating option_3 with enable_timeout=True
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
  |-> step_number: 400
  |-> step_number: 500
  |-> step_number: 600
  |-> step_number: 700
option_2 execution successful
option_2 execution successful
Episode 25	Average Score: -857.80	Duration: 858.30 steps	GO Eps: 0.79
|-> episode: 26
  |-> step_number: 0
    |-> (Option::train): call (train option_3)
      |-> num_goal_hits: 1, num_subgoal_hits_required: 5 
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
overall_goal_policy_option execution successful
Episode 26	Average Score: -779.60	Duration: 780.20 steps	GO Eps: 0.78
|-> episode: 27
  |-> step_number: 0
    |-> (Option::train): call (train option_3)
      |-> num_goal_hits: 2, num_subgoal_hits_required: 5 
option_2 execution successful
option_2 execution successful
option_2 execution successful
  |-> step_number: 200
  |-> step_number: 300
overall_goal_policy_option execution successful
Episode 27	Average Score: -738.40	Duration: 739.00 steps	GO Eps: 0.78
|-> episode: 28
  |-> step_number: 0
    |-> (Option::train): call (train option_3)
      |-> num_goal_hits: 3, num_subgoal_hits_required: 5 
option_2 execution successful
option_2 execution successful
option_1 execution successful
option_1 execution successful
option_1 execution successful
option_1 execution successful
option_1 execution successful
option_1 execution successful
option_1 execution successful
option_1 execution successful
option_1 execution successful
  |-> step_number: 400
option_1 execution successful
option_1 execution successful
option_1 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
Episode 28	Average Score: -738.40	Duration: 739.00 steps	GO Eps: 0.77
|-> episode: 29
  |-> step_number: 0
    |-> (Option::train): call (train option_3)
      |-> num_goal_hits: 4, num_subgoal_hits_required: 5 
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
  |-> step_number: 300
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
  |-> step_number: 500
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
  |-> step_number: 700
option_2 execution successful
  |-> step_number: 900
Episode 29	Average Score: -738.40	Duration: 739.00 steps	GO Eps: 0.76
|-> episode: 30
  |-> step_number: 0
    |-> (Option::train): call (train option_3)
      |-> num_goal_hits: 5, num_subgoal_hits_required: 5 
      |-> No negative examples...Adding 5 now!
option_3 execution successful
option_3 execution successful
option_3 execution successful
option_3 execution successful
option_3 execution successful

Creating GlobalDQN with lr=0.0001 and ddqn=True and buffer_sz=1000000

Initializing new option node with q value 0.0
Init state is in option_3's initiation set classifier
option_2 execution successful
[option_1]: x: 1.3194645468039459	y: 8.663898596206977	theta: 5.574081886609706	xdot: -6.686647079711936	ydot: 0.10004024007016712	thetadot: 9.56524378911009	terminal: False
 is_terminal() but not term_true()
Episode 30	Average Score: -666.70	Duration: 667.30 steps	GO Eps: 0.76
Episode 30	Average Score: -666.70	Duration: 667.30 steps	GO Eps: 0.76
|-> episode: 31
  |-> step_number: 0
  |-> step_number: 100
option_3 execution successful
option_3 execution successful
option_3 execution successful
option_3 execution successful
option_3 execution successful
option_3 execution successful
option_3 execution successful
option_1 execution successful
option_1 execution successful
  |-> step_number: 600
option_1 execution successful
option_1 execution successful
option_1 execution successful
option_1 execution successful
Episode 31	Average Score: -675.20	Duration: 675.70 steps	GO Eps: 0.75
|-> episode: 32
  |-> step_number: 0
  |-> step_number: 100
option_3 execution successful
option_3 execution successful
option_3 execution successful
option_3 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
  |-> step_number: 600
  |-> step_number: 800
option_1 execution successful
[option_1]: x: 1.24022049644281	y: 7.255396468711194	theta: -31.018299717303385	xdot: 0.2037269688871871	ydot: -5.537750704355001	thetadot: 9.964008369467884	terminal: False
 is_terminal() but not term_true()
Episode 32	Average Score: -671.80	Duration: 672.30 steps	GO Eps: 0.74
|-> episode: 33
  |-> step_number: 0
option_3 execution successful
option_3 execution successful
option_3 execution successful
option_3 execution successful
option_3 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
  |-> step_number: 600
option_2 execution successful
option_2 execution successful
option_2 execution successful
  |-> step_number: 700
global_option execution successful
Episode 33	Average Score: -742.70	Duration: 743.20 steps	GO Eps: 0.73
|-> episode: 34
  |-> step_number: 0
option_3 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
  |-> step_number: 400
option_2 execution successful
  |-> step_number: 700
  |-> step_number: 800
  |-> step_number: 900
Episode 34	Average Score: -742.70	Duration: 743.20 steps	GO Eps: 0.72
|-> episode: 35
  |-> step_number: 0
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
Episode 35	Average Score: -742.70	Duration: 743.20 steps	GO Eps: 0.71
|-> episode: 36
  |-> step_number: 0
option_3 execution successful
option_3 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
  |-> step_number: 500
option_1 execution successful
overall_goal_policy_option execution successful
Episode 36	Average Score: -785.20	Duration: 785.70 steps	GO Eps: 0.71
|-> episode: 37
  |-> step_number: 0
option_2 execution successful
option_2 execution successful
option_2 execution successful
  |-> step_number: 400
option_1 execution successful
option_1 execution successful
  |-> step_number: 800
option_2 execution successful
option_2 execution successful
Episode 37	Average Score: -853.80	Duration: 854.20 steps	GO Eps: 0.70
|-> episode: 38
  |-> step_number: 0
option_2 execution successful
option_3 execution successful
option_2 execution successful
option_2 execution successful
  |-> step_number: 400
option_2 execution successful
option_2 execution successful
option_1 execution successful
option_1 execution successful
[option_1]: x: -0.8225970755838232	y: 8.034430561711332	theta: 21.167464693242753	xdot: 5.435188715237363	ydot: -2.9668586991440904	thetadot: 7.3504575315690195	terminal: False
 is_terminal() but not term_true()
Episode 38	Average Score: -847.70	Duration: 848.20 steps	GO Eps: 0.69
|-> episode: 39
  |-> step_number: 0
option_1 execution successful
  |-> step_number: 200
option_1 execution successful
option_1 execution successful
option_1 execution successful
option_1 execution successful
option_1 execution successful
option_1 execution successful
  |-> step_number: 400
option_2 execution successful
option_2 execution successful
option_2 execution successful
  |-> step_number: 700
overall_goal_policy_option execution successful
Episode 39	Average Score: -822.60	Duration: 823.20 steps	GO Eps: 0.68
|-> episode: 40
  |-> step_number: 0
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
  |-> step_number: 500
  |-> step_number: 600
overall_goal_policy_option execution successful
Episode 40	Average Score: -877.40	Duration: 878.00 steps	GO Eps: 0.67
Episode 40	Average Score: -877.40	Duration: 878.00 steps	GO Eps: 0.67
|-> episode: 41
  |-> step_number: 0
option_2 execution successful
option_2 execution successful
option_1 execution successful
option_1 execution successful
[option_1]: x: 1.0632381065511027	y: 7.360918652773167	theta: 37.71684211091464	xdot: -4.38724388627677	ydot: -3.784649225045285	thetadot: 5.060245146557376	terminal: False
 is_terminal() but not term_true()
Episode 41	Average Score: -833.50	Duration: 834.20 steps	GO Eps: 0.67
|-> episode: 42
  |-> step_number: 0
option_3 execution successful
option_3 execution successful
option_3 execution successful
option_3 execution successful
  |-> step_number: 200
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_1 execution successful
option_1 execution successful
option_1 execution successful
option_1 execution successful
option_1 execution successful
option_1 execution successful
option_1 execution successful
  |-> step_number: 800
option_1 execution successful
option_1 execution successful
[option_1]: x: 0.6564422131087224	y: 9.324290061235791	theta: 38.310610248984545	xdot: 0.11997826829223421	ydot: -8.699178244571016	thetadot: 8.033779842445519	terminal: False
 is_terminal() but not term_true()
Episode 42	Average Score: -827.50	Duration: 828.20 steps	GO Eps: 0.66
|-> episode: 43
  |-> step_number: 0
option_2 execution successful
option_1 execution successful
option_1 execution successful
  |-> step_number: 300
option_2 execution successful
option_2 execution successful
option_2 execution successful
  |-> step_number: 400
option_1 execution successful
  |-> step_number: 600
option_1 execution successful
option_1 execution successful
option_1 execution successful
[option_1]: x: 0.32865333248289014	y: 6.793035806998618	theta: 24.393456686191904	xdot: 0.5212902422417532	ydot: 4.436067241887318	thetadot: -1.5665474078791028	terminal: False
 is_terminal() but not term_true()
Episode 43	Average Score: -840.20	Duration: 840.90 steps	GO Eps: 0.65
|-> episode: 44
  |-> step_number: 0
overall_goal_policy_option execution successful
Episode 44	Average Score: -763.70	Duration: 764.50 steps	GO Eps: 0.65
|-> episode: 45
  |-> step_number: 0
option_2 execution successful
option_2 execution successful
  |-> step_number: 100
option_2 execution successful
option_2 execution successful
  |-> step_number: 200
option_1 execution successful
option_1 execution successful
option_1 execution successful
  |-> step_number: 400
option_1 execution successful
option_2 execution successful
option_2 execution successful
  |-> step_number: 700
  |-> step_number: 800
option_1 execution successful
Episode 45	Average Score: -763.70	Duration: 764.50 steps	GO Eps: 0.64
|-> episode: 46
  |-> step_number: 0
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
overall_goal_policy_option execution successful
Episode 46	Average Score: -775.90	Duration: 776.70 steps	GO Eps: 0.63
|-> episode: 47
  |-> step_number: 0
option_1 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_1 execution successful
option_1 execution successful
[option_1]: x: 0.8790416442889346	y: 7.65614114817357	theta: 12.510934336997742	xdot: 0.04413869620153305	ydot: -1.9603265526551268	thetadot: 0.0339417887585588	terminal: False
 is_terminal() but not term_true()
Episode 47	Average Score: -751.20	Duration: 752.10 steps	GO Eps: 0.62
|-> episode: 48
  |-> step_number: 0
option_2 execution successful
  |-> step_number: 200
overall_goal_policy_option execution successful
Episode 48	Average Score: -693.20	Duration: 694.10 steps	GO Eps: 0.62
|-> episode: 49
  |-> step_number: 0
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_1 execution successful
option_1 execution successful
option_1 execution successful
  |-> step_number: 400
option_1 execution successful
overall_goal_policy_option execution successful
Episode 49	Average Score: -689.30	Duration: 690.20 steps	GO Eps: 0.61
|-> episode: 50
  |-> step_number: 0
option_1 execution successful
option_1 execution successful
  |-> step_number: 600
  |-> step_number: 800
option_1 execution successful
Episode 50	Average Score: -721.00	Duration: 721.80 steps	GO Eps: 0.60
Episode 50	Average Score: -721.00	Duration: 721.80 steps	GO Eps: 0.60
|-> episode: 51
  |-> step_number: 0
option_2 execution successful
  |-> step_number: 300
  |-> step_number: 400
  |-> step_number: 500
  |-> step_number: 600
  |-> step_number: 700
  |-> step_number: 800
overall_goal_policy_option execution successful
Episode 51	Average Score: -750.70	Duration: 751.50 steps	GO Eps: 0.59
|-> episode: 52
  |-> step_number: 0
option_3 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
[option_1]: x: 1.037956206182377	y: 7.430673120683875	theta: 50.52322015650012	xdot: -1.4796217616201683	ydot: -4.856509181043662	thetadot: 0.6539429208694185	terminal: False
 is_terminal() but not term_true()
Episode 52	Average Score: -711.50	Duration: 712.30 steps	GO Eps: 0.59
|-> episode: 53
  |-> step_number: 0
option_2 execution successful
option_2 execution successful
option_2 execution successful
  |-> step_number: 300
option_2 execution successful
  |-> step_number: 500
option_1 execution successful
option_1 execution successful
option_1 execution successful
option_1 execution successful
option_1 execution successful
option_1 execution successful
  |-> step_number: 800
Episode 53	Average Score: -713.40	Duration: 714.10 steps	GO Eps: 0.58
|-> episode: 54
  |-> step_number: 0
option_3 execution successful
option_3 execution successful
option_1 execution successful
  |-> step_number: 300
overall_goal_policy_option execution successful
Episode 54	Average Score: -729.10	Duration: 729.80 steps	GO Eps: 0.57
|-> episode: 55
  |-> step_number: 0
option_2 execution successful
option_2 execution successful
option_2 execution successful
  |-> step_number: 300
  |-> step_number: 400
  |-> step_number: 500
  |-> step_number: 600
overall_goal_policy_option execution successful
Episode 55	Average Score: -695.20	Duration: 696.00 steps	GO Eps: 0.57
|-> episode: 56
  |-> step_number: 0
option_1 execution successful
option_1 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
  |-> step_number: 500
option_2 execution successful
option_1 execution successful
option_2 execution successful
option_2 execution successful
Episode 56	Average Score: -718.70	Duration: 719.40 steps	GO Eps: 0.56
|-> episode: 57
  |-> step_number: 0
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_1 execution successful
option_2 execution successful
overall_goal_policy_option execution successful
Episode 57	Average Score: -742.40	Duration: 743.10 steps	GO Eps: 0.55
|-> episode: 58
  |-> step_number: 0
option_2 execution successful
  |-> step_number: 100
  |-> step_number: 200
  |-> step_number: 500
option_2 execution successful
option_2 execution successful
option_2 execution successful
option_2 execution successful
Episode 58	Average Score: -806.50	Duration: 807.10 steps	GO Eps: 0.54
|-> episode: 59
  |-> step_number: 0
  |-> step_number: 300
overall_goal_policy_option execution successful
Episode 59	Average Score: -800.00	Duration: 800.60 steps	GO Eps: 0.53
|-> episode: 60
  |-> step_number: 0
option_2 execution successful
option_1 execution successful
  |-> step_number: 100
  |-> step_number: 200
overall_goal_policy_option execution successful
Episode 60	Average Score: -732.00	Duration: 732.70 steps	GO Eps: 0.53
Episode 60	Average Score: -732.00	Duration: 732.70 steps	GO Eps: 0.53
|-> episode: 61
  |-> step_number: 0
option_3 execution successful
OptionClass::execute_option_in_mdp: option_transition is empty!
Traceback (most recent call last):
  File "simple_rl/agents/func_approx/dsc/SkillChainingAgentClass.py", line 903, in <module>
    episodic_scores, episodic_durations = chainer.skill_chaining(args.episodes, args.steps)
  File "simple_rl/agents/func_approx/dsc/SkillChainingAgentClass.py", line 665, in skill_chaining
    state, step_number, episode_option_executions, episode)
  File "simple_rl/agents/func_approx/dsc/SkillChainingAgentClass.py", line 292, in take_action
    next_state = self.get_next_state_from_experiences(option_transitions)
  File "simple_rl/agents/func_approx/dsc/SkillChainingAgentClass.py", line 324, in get_next_state_from_experiences
    return experiences[-1][-1]
IndexError: list index out of range
