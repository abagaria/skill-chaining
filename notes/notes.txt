Conda
=====
	conda env export --no-builds > environment2.yml
	conda env create -f=environment2.yml

	python -m ipykernel install --user --name dsc --display-name "Python (dsc)"

	Experience Format (s, a, r, s'):
		(
			x: 0.04563510852011676	y: 0.025069855877261793 has_key: False theta: 0.020345420899034905 xdot: 0.027840619382801225 ydot: -0.0870369329932777 thetadot: -0.08814207475220448 terminal: False,
			array([1., 0.36957714], dtype=float32),
			-1.0,
			x: 0.9728018368897211	y: 0.3982240908521795	has_key: False	theta: 0.3828711945348712	xdot: 0.027865134602680385ydot: -0.08702695899573652	thetadot: -0.08814207475220448	terminal: False
		)

Run
===
	# test (CPU)
	python3 simple_rl/agents/func_approx/dsc/SkillChainingAgentClass.py --env="maze" --experiment_name="1 - num_sub_goal_hits=1, nu=0.8" --episodes=100 --steps=2000 --use_smdp_update=True --option_timeout=True --subgoal_reward=300. --buffer_len=100 --device="cpu" --num_subgoal_hits=1 --nu=0.8

	# test (GPU)
	python3 simple_rl/agents/func_approx/dsc/SkillChainingAgentClass.py --env="maze" --experiment_name="1 - num_sub_goal_hits=5, nu=0.8" --episodes=1000 --steps=2000 --use_smdp_update=True --option_timeout=True --subgoal_reward=300. --buffer_len=100 --device="cuda:0" --num_subgoal_hits=5 --nu=0.8

	# baseline
	python3 simple_rl/agents/func_approx/dsc/SkillChainingAgentClass.py --env="maze" --experiment_name="sc_opt_pes_test" --episodes=2000 --steps=2000 --use_smdp_update=True --option_timeout=True --subgoal_reward=300. --buffer_len=20 --device="cpu" --num_subgoal_hits=3 --nu=0.5

	# Debugging
	python3 simple_rl/agents/func_approx/dsc/SkillChainingAgentClass.py --env="maze" --experiment_name="(debug) num_sub_goal_hits=5, nu=0.7" --episodes=300 --steps=1000 --use_smdp_update=True --option_timeout=True --subgoal_reward=300. --buffer_len=100 --device="cuda:0" --num_subgoal_hits=5 --nu=0.7

Bugs
====
	[ ] Treasure Game:
		- seed=0, steps=8000:
			* 3400 steps: reaches gold coin
			* 7700 steps: reaches goal

		- the DQN agent eps decays very fast (almost to 0 in 10 episodes)
			* this could due to DQN not performing well for extended temporal planning
		
	[x] Test failed unexpectedly due to option_transitions list being empty
		- Command:
			
			python3 -u simple_rl/agents/func_approx/dsc/SkillChainingAgentClass.py --env="maze" --experiment_name='(cs2951x) dsc_opt_pes_nu_0.5' --episodes=300 --steps=1000 --use_smdp_update=True --option_timeout=True --subgoal_reward=300. --buffer_len=100 --device="cuda:0" --num_subgoal_hits=5 --nu=0.5 --num_run=4 --seed=4

		- Error message:

			Traceback (most recent call last):
			File "simple_rl/agents/func_approx/dsc/SkillChainingAgentClass.py", line 903, in <module>
				episodic_scores, episodic_durations = chainer.skill_chaining(args.episodes, args.steps)
			File "simple_rl/agents/func_approx/dsc/SkillChainingAgentClass.py", line 665, in skill_chaining
				state, step_number, episode_option_executions, episode)
			File "simple_rl/agents/func_approx/dsc/SkillChainingAgentClass.py", line 292, in take_action
				next_state = self.get_next_state_from_experiences(option_transitions)
			File "simple_rl/agents/func_approx/dsc/SkillChainingAgentClass.py", line 324, in get_next_state_from_experiences
				return experiences[-1][-1]
			IndexError: list index out of range

		- Cause?
			- trying to execute option from a state that was previously in termination set of option

		- seems to happen more with the higher nu is
		-> a lot of FAILED tests when nu=0.8, some with nu=0.7, a couple with nu=0.6

		- is_term_true(state) is returning different answers for almost the same state. maybe a rounding issue?

		e.g
			state (np.array) = [0.32291666 -0.00160256], is_term_true(state) = False
			state (list) = [0.3229166666666667, -0.0016025641025641025], is_term_true(state) = True

		- FIX:
			- torch and numpy use different rounding schemes
			
			i.e. for a given state,
				
				torch.from_numpy(state).float().unsqueeze(0).to(self.device)

				is not equivalent to

				np.array(state)
			
			- use make sure anything passed to is_term_true(state) is a np.array 

	[ ] 1st option has init state and never learns more options using probabilistc methods
		- Q: maybe the init set of the goal policy option is too large at first but then is refined 
		down but if an option is created early then this will break?
		
		- Akhil: buff len too large, try smaller buffer

		- Hyperparameters

			episodes=100
			steps=2000
			subgoal_reward=300
			buffer_len=20
			num_sub_goal_hits=1
			nu=0.5
			seed=0
		
		- test run

			python3 simple_rl/agents/func_approx/dsc/SkillChainingAgentClass.py --env="maze" --experiment_name="(debug) predict()" --episodes=300 --steps=1000 --use_smdp_update=True --option_timeout=True --subgoal_reward=300. --buffer_len=20 --device="cuda:0" --num_subgoal_hits=1 --nu=0.5 --episodic_plots=True
			python3 simple_rl/agents/func_approx/dsc/SkillChainingAgentClass.py --env="maze" --experiment_name="(debug) prob_init" --episodes=300 --steps=1000 --use_smdp_update=True --option_timeout=True --subgoal_reward=300. --buffer_len=20 --device="cuda:0" --num_subgoal_hits=1 --nu=0.5 --episodic_plots=True

		- Testing:
			[o] set nu=0.7: only created 1 option
			[o] set num_sub_goal_hits=5, nu=0.7: only created 1 option
			[x] init=opt clf, terminate=parent init: more options have been created!
			[o] init=prob(opt clf) and terminate=parent init: creating 1 option
				-> issue is with proba init?
			[x] (1) init=opt clf, terminate=pes clf: more options have been created!
			[x] (2) init=opt clf, terminate=pes clf: more options have beem created!
				-> confirmed that proba init is the issue and not term
			[x/o] set dsc seed=1, init=prob(opt clf), terminate=pes clf: two options created but fairly near init state
			[o] set dsc seed=2, init=prob(opt clf), terminate=pes clf: only created 1 option
			[o] set dsc seed=3, init=prob(opt clf), terminate=pes clf: goal option only created
			[o] set dsc seed=4, init=prob(opt clf), terminate=pes clf: 1 option created but goal option has init state
			[o] set dsc seed=4, init=prob(opt clf), terminate=pes clf, num_sub_goal_hits=3: 1 option created, goal option and option 1 has init state
				-> increasing gestation period doesn't help
				-> overall, proba init is the issue across diff seeds
				-> need to figure out the main difference
			
			Q: what's the difference between using opt_clf.predict() and opt_clf.predict_proba()?
				- opt_clf.predict(): returns TRUE if classifier at a given state is POS
				- opt_clf.predict_proba(): returns TRUE if probability of classifier of POS predicted state is higher than a random probability distribution

			[o] seed=1, a) init=opt_clf.predict(), terminate=pes_clf.predict(), b) init=opt_clf.predict_proba(), terminate=pes_clf.predict():
				- both a) and b) have goal option takes up top half, option 1 has init state
				-> bad seed?
			[x] seed=0; a) init=opt_clf.predict(), terminate=pes_clf.predict(), b) init=opt_clf.predict_proba(), terminate=pes_clf.predict():
				- a) more options created that spread the domain! 
				- b) only created option 1
			
				- both a) and b) goal option are reletively the same
				- option 1 for a) is bottom right while for b) it is bottom left (around init state)

			Q: could using probabilistc init be only working for an option around init state? Why?

			* Seeding for prob init might have been wrong but fixed now

			[o] seed=0, init=prob(opt clf), term=pes clf, added batch term, updated batch init in DQN:
				- still only creates 1 option but is top right and not in init state
				-> is there a different bug in creating options?
			[o] (1) seed=0, init=prob(opt clf), term=pes clf, setting seed before: only created 1 option in init state 
			[o] (2) seed=0, init=prob(opt clf), term=pes clf, setting seed before: only created 1 option in init state

			Q: what does the first option look like when created? why is it only being created around the initial state?

			[o] seed=0, init=prob(opt clf), term=pes clf:
				- when created, only successful trajectories collected are around initial state.
				-> why?

			[o] seed=1, init=prob(opt clf), term=pes clf:
				- again, successful trajectories are only around initial state
				-> maybe look into how trajectories are collected per option?

			[o] seed=2, init=prob(opt clf), term=pes clf:
				- probability values are very low most likely due to the fact that there are so little
				positive samples but this is expected
				-> how would this affect only creating an option around the initial state?
			
			Q: when first option is created, why are all the successful trajectories around the initial state with prob init
			   but when predict is used, they aren't?
				- Somehow, is_init_true() is affecting the samples being collected during gestation

Structure
=========
	{Class}::{Method}

	SkillChainingAgentClass::skill_chaining(...)
	|- ...
	|- SkillChainingAgentClass::take_action(...)
		|- ...
		|- OptionClass::execute_option_in_mdp(...)
		|- ...
		|- OptionClass::refine_initiation_set_classifier(...)
			|- ...
			|- OptionClass::train_initiation_set_classifier()
	|- ...
	|- OptionClass::train(...)
		|- ...
		|- OptionClass::train_initiation_set_classifier()

Hyperparameter search
=====================

	Run
	---
	python3 simple_rl/agents/func_approx/dsc/SkillChainingAgentClass.py --env="maze" --experiment_name="(test) baseline" --episodes=300 --steps=1000 --use_smdp_update=True --option_timeout=True --subgoal_reward=300. --buffer_len=20 --device="cuda:0" --num_subgoal_hits=5 --nu=0.5 

	Hyperparameters
	---------------
	- episodes
	- steps
	- num_sub_goal_hits: size of gestation period
	- nu: % of how conservative the OneClassSVM (i.e. if nu=0.7 then will treat 70% of data as outlier)
	- subgoal_reward
	- buffer_len: can't be too big

	Defaults (Point Maze)
	---------------------
	- episodes = 300
	- steps = 1000
	- subgoal_reward = 300
	- buffer_len = 20
	- num_sub_goal_hits: 5
	- opt_nu = 0.1

	Search
	---------------
	- pes_nu: 0.5, 0.6, 0.7, 0.8

