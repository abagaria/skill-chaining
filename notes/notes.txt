Conda
=====
	conda env export --no-builds > environment2.yml
	conda env create -f=environment2.yml

	python -m ipykernel install --user --name dsc --display-name "Python (dsc)"

	Experience Format (s, a, r, s'):
		(
			x: 0.04563510852011676	y: 0.025069855877261793 has_key: False theta: 0.020345420899034905 xdot: 0.027840619382801225 ydot: -0.0870369329932777 thetadot: -0.08814207475220448 terminal: False,
			array([1., 0.36957714], dtype=float32),
			-1.0,
			x: 0.9728018368897211	y: 0.3982240908521795	has_key: False	theta: 0.3828711945348712	xdot: 0.027865134602680385ydot: -0.08702695899573652	thetadot: -0.08814207475220448	terminal: False
		)

Run
===
	# test (CPU)
	python3 simple_rl/agents/func_approx/dsc/SkillChainingAgentClass.py --env="maze" --experiment_name="1 - num_sub_goal_hits=1, nu=0.8" --episodes=100 --steps=2000 --use_smdp_update=True --option_timeout=True --subgoal_reward=300. --buffer_len=100 --device="cpu" --num_subgoal_hits=1 --nu=0.8

	# test (GPU)
	python3 simple_rl/agents/func_approx/dsc/SkillChainingAgentClass.py --env="maze" --experiment_name="1 - num_sub_goal_hits=5, nu=0.8" --episodes=1000 --steps=2000 --use_smdp_update=True --option_timeout=True --subgoal_reward=300. --buffer_len=100 --device="cuda:0" --num_subgoal_hits=5 --nu=0.8

	# baseline
	python3 simple_rl/agents/func_approx/dsc/SkillChainingAgentClass.py --env="maze" --experiment_name="sc_opt_pes_test" --episodes=2000 --steps=2000 --use_smdp_update=True --option_timeout=True --subgoal_reward=300. --buffer_len=20 --device="cpu" --num_subgoal_hits=3 --nu=0.5

	# Debugging
	python3 simple_rl/agents/func_approx/dsc/SkillChainingAgentClass.py --env="maze" --experiment_name="(debug) num_sub_goal_hits=5, nu=0.7" --episodes=300 --steps=1000 --use_smdp_update=True --option_timeout=True --subgoal_reward=300. --buffer_len=100 --device="cuda:0" --num_subgoal_hits=5 --nu=0.7

Bugs
====
	[ ] Tunning for Treasure Game:
		- seed=0, steps=8000:
			* 3400 steps: reaches gold coin
			* 7700 steps: reaches goal

		- the DQN agent eps decays very fast (almost to 0 in 10 episodes)
			* this could due to DQN not performing well for extended temporal planning
		
	[x] Test failed unexpectedly due to option_transitions list being empty
		- Command:
			
			python3 -u simple_rl/agents/func_approx/dsc/SkillChainingAgentClass.py --env="maze" --experiment_name='(cs2951x) dsc_opt_pes_nu_0.5' --episodes=300 --steps=1000 --use_smdp_update=True --option_timeout=True --subgoal_reward=300. --buffer_len=100 --device="cuda:0" --num_subgoal_hits=5 --nu=0.5 --num_run=4 --seed=4

		- Error message:

			Traceback (most recent call last):
			File "simple_rl/agents/func_approx/dsc/SkillChainingAgentClass.py", line 903, in <module>
				episodic_scores, episodic_durations = chainer.skill_chaining(args.episodes, args.steps)
			File "simple_rl/agents/func_approx/dsc/SkillChainingAgentClass.py", line 665, in skill_chaining
				state, step_number, episode_option_executions, episode)
			File "simple_rl/agents/func_approx/dsc/SkillChainingAgentClass.py", line 292, in take_action
				next_state = self.get_next_state_from_experiences(option_transitions)
			File "simple_rl/agents/func_approx/dsc/SkillChainingAgentClass.py", line 324, in get_next_state_from_experiences
				return experiences[-1][-1]
			IndexError: list index out of range

		- Cause?
			- trying to execute option from a state that was previously in termination set of option

		- seems to happen more with the higher nu is
		-> a lot of FAILED tests when nu=0.8, some with nu=0.7, a couple with nu=0.6

		- is_term_true(state) is returning different answers for almost the same state. maybe a rounding issue?

		e.g
			state (np.array) = [0.32291666 -0.00160256], is_term_true(state) = False
			state (list) = [0.3229166666666667, -0.0016025641025641025], is_term_true(state) = True

		- FIX:
			- torch and numpy use different rounding schemes
			
			i.e. for a given state,
				
				torch.from_numpy(state).float().unsqueeze(0).to(self.device)

				is not equivalent to

				np.array(state)
			
			- use make sure anything passed to is_term_true(state) is a np.array 

Structure
=========
	{Class}::{Method}

	SkillChainingAgentClass::skill_chaining(...)
	|- ...
	|- SkillChainingAgentClass::take_action(...)
		|- ...
		|- OptionClass::execute_option_in_mdp(...)
		|- ...
		|- OptionClass::refine_initiation_set_classifier(...)
			|- ...
			|- OptionClass::train_initiation_set_classifier()
	|- ...
	|- OptionClass::train(...)
		|- ...
		|- OptionClass::train_initiation_set_classifier()

Hyperparameter search
=====================

	Run
	---
	python3 simple_rl/agents/func_approx/dsc/SkillChainingAgentClass.py --env="maze" --experiment_name="(test) baseline" --episodes=300 --steps=1000 --use_smdp_update=True --option_timeout=True --subgoal_reward=300. --buffer_len=20 --device="cuda:0" --num_subgoal_hits=5 --nu=0.5 

	Hyperparameters
	---------------
	- episodes
	- steps
	- num_sub_goal_hits: size of gestation period
	- nu: % of how conservative the OneClassSVM (i.e. if nu=0.7 then will treat 70% of data as outlier)
	- subgoal_reward
	- buffer_len: can't be too big

	Defaults (Treaure Game)
	---------------------
	- episodes = ???
	- steps = ???
	- subgoal_reward = 300
	- buffer_len = 20
	- num_sub_goal_hits: 5
	- opt_nu = 0.1

	Search
	---------------
	- pes_nu: 0.1 - 0.9

Future Work
===========

* Why an increase of variance with continuous learning?

	- Map chain execution when rewards took a hit 

		(cs2951x) maze_pes_nu_0.3_with_chainfix/run_1.log

			Episode 290	Score: -420.00	Average Score: -611.30	Duration: 612.30 steps	GO Eps: 0.00
			Current Skill Chain: [option_1, option_3, option_2, option_5, option_4, option_6, option_7]
				option_2 execution successful
				option_3 execution successful
				option_2 execution successful
				option_1 execution successful
		
			Episode 291	Score: -655.00	Average Score: -626.50	Duration: 627.50 steps	GO Eps: 0.00
			Current Skill Chain: [option_1, option_3, option_2, option_5, option_4, option_6, option_7]
				option_3 execution successful
				option_3 execution successful
				option_3 execution successful
				option_3 execution successful
				global_option execution successful

			Episode 292	Score: -73.00	Average Score: -563.00	Duration: 564.00 steps	GO Eps: 0.00
			Current Skill Chain: [option_1, option_3, option_2, option_5, option_4, option_6, option_7]
				option_6 execution successful

			Episode 293	Score: -912.00	Average Score: -584.30	Duration: 585.30 steps	GO Eps: 0.00
			Current Skill Chain: [option_1, option_3, option_2, option_5, option_4, option_6, option_7]
				option_4 execution successful
				option_3 execution successful
				option_3 execution successful
				option_3 execution successful
				option_1 execution successful

			Episode 294	Score: -2000.00	Average Score: -714.70	Duration: 715.60 steps	GO Eps: 0.00
			Current Skill Chain: [option_1, option_3, option_2, option_5, option_4, option_6, option_7]
				option_7 execution successful
				option_6 execution successful
				option_4 execution successful

			Episode 295	Score: -1099.00	Average Score: -755.20	Duration: 756.10 steps	GO Eps: 0.00
			Current Skill Chain: [option_1, option_3, option_2, option_5, option_4, option_6, option_7]
				option_6 execution successful
				option_6 execution successful
				option_3 execution successful
				option_3 execution successful
				option_1 execution successful

* Exploration epsilon schedule needs to be more dynamic to domain

* How do chain fix options hold up to longer horizon tasks (i.e. treasure game)?

